{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# Video Translator Notebook per Google Colab\n",
        "\n",
        "Questo notebook consente di eseguire il processo di traduzione video con clonazione vocale. (//TODO: Lip Sync)\n",
        "\n",
        "Assicurati di avere i file necessari (video in input e, se desiderato, campioni vocali in formato WAV) e che la runtime disponga di una GPU per ottenere prestazioni ottimali.\n",
        "\n",
        "Il codice è stato sviluppato da Mike Gazzaruso ed è rilasciato sotto licenza GNU/GPL v3."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Per prima cosa, controlliamo se l'istanza è privvista di GPU. Si consiglia di utilizzare un'istanza con GPU perché i calcoli sarebbero estremamente lenti sfruttando solamente la CPU."
      ],
      "metadata": {
        "id": "Pk6yVu7yMMPu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpu-check"
      },
      "outputs": [],
      "source": [
        "# Controllo della disponibilità di GPU\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU disponibile! Utilizzo della GPU.\")\n",
        "else:\n",
        "    print(\"GPU non disponibile. Si consiglia l'utilizzo di una runtime con GPU per prestazioni migliori.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Procediamo ora a disinstallare eventuali pacchetti già installati sull'istanza e ad installare le versioni specifiche dei pacchetti che ci interessano."
      ],
      "metadata": {
        "id": "_gLGXar3Ma20"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-deps"
      },
      "outputs": [],
      "source": [
        "# Rimuovi eventuali versioni di sentence-transformers, transformers e tokenizers già installate\n",
        "!pip uninstall -y sentence-transformers transformers tokenizers\n",
        "\n",
        "# Aggiorna pip\n",
        "!pip install --upgrade pip\n",
        "\n",
        "# Installa le dipendenze principali\n",
        "!pip install torch torchvision torchaudio --quiet\n",
        "!pip install librosa moviepy openai-whisper tortoise-tts pydub opencv-python soundfile demucs --quiet\n",
        "\n",
        "# Installa transformers==4.31.0, che installerà automaticamente una versione compatibile di tokenizers\n",
        "!pip install transformers==4.31.0 --quiet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import-libraries-md"
      },
      "source": [
        "## Importazione delle librerie\n",
        "\n",
        "In questa cella importiamo tutte le librerie utilizzate dallo script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "import-libraries-code"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "from moviepy.editor import VideoFileClip, AudioFileClip\n",
        "import whisper\n",
        "from transformers import pipeline, MBartForConditionalGeneration, MBart50TokenizerFast\n",
        "from tortoise.api import TextToSpeech\n",
        "from tortoise.utils.audio import load_audio, load_voice\n",
        "import cv2\n",
        "from pydub import AudioSegment\n",
        "import tempfile\n",
        "import shutil\n",
        "import time\n",
        "import argparse\n",
        "import sys\n",
        "import datetime\n",
        "import soundfile as sf\n",
        "import warnings\n",
        "import traceback\n",
        "import json\n",
        "import hashlib\n",
        "import pickle\n",
        "import demucs.separate\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "video-translator-md"
      },
      "source": [
        "## Definizione della classe `VideoTranslator`\n",
        "\n",
        "La seguente cella contiene la definizione completa della classe `VideoTranslator` con tutti i metodi necessari per:\n",
        "- Inizializzare i modelli\n",
        "- Estrarre e trascrivere l'audio\n",
        "- Tradurre il testo\n",
        "- Generare segmenti audio con voce clonata\n",
        "- Combinare i segmenti audio e il video originale\n",
        "\n",
        "Il codice è ampiamente commentato per una migliore comprensione."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "video-translator-code"
      },
      "outputs": [],
      "source": [
        "class VideoTranslator:\n",
        "    def __init__(self, source_lang=\"it\", target_lang=\"en\", voice_samples_dir=None, input_video_path=None, use_cache=True):\n",
        "        \"\"\"\n",
        "        Inizializza il traduttore video.\n",
        "\n",
        "        Args:\n",
        "            source_lang: Lingua sorgente (default: Italiano)\n",
        "            target_lang: Lingua target (default: Inglese)\n",
        "            voice_samples_dir: Directory contenente campioni vocali per il clonaggio\n",
        "            input_video_path: Percorso del video in input per creare una directory dedicata\n",
        "            use_cache: Se usare la cache per campioni e modelli (default: True)\n",
        "        \"\"\"\n",
        "        self.source_lang = source_lang\n",
        "        self.target_lang = target_lang\n",
        "        self.voice_samples_dir = voice_samples_dir\n",
        "        self.use_cache = use_cache\n",
        "\n",
        "        # Creazione di una directory dedicata per le conversioni\n",
        "        if input_video_path:\n",
        "            video_filename = os.path.basename(input_video_path).split('.')[0]\n",
        "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            work_dir_name = f\"{video_filename}_{timestamp}\"\n",
        "\n",
        "            # Nei notebook non esiste __file__, usa os.getcwd()\n",
        "            script_dir = os.getcwd()\n",
        "            self.temp_dir = os.path.join(script_dir, \"conversions\", work_dir_name)\n",
        "            os.makedirs(self.temp_dir, exist_ok=True)\n",
        "            print(f\"Working directory created: {self.temp_dir}\")\n",
        "        else:\n",
        "            self.temp_dir = tempfile.mkdtemp()\n",
        "\n",
        "        # Setup della directory di cache\n",
        "        script_dir = os.getcwd()\n",
        "        self.cache_dir = os.path.join(script_dir, \"cache\")\n",
        "        os.makedirs(self.cache_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "        # Debug: controllo della directory dei campioni vocali\n",
        "        if voice_samples_dir:\n",
        "            print(f\"Voice samples directory specified: {voice_samples_dir}\")\n",
        "            if os.path.exists(voice_samples_dir):\n",
        "                print(f\"Directory exists!\")\n",
        "                wav_files = [f for f in os.listdir(voice_samples_dir) if f.endswith('.wav')]\n",
        "                print(f\"WAV files found: {len(wav_files)}\")\n",
        "                if wav_files:\n",
        "                    print(f\"Examples: {wav_files[:3]}\")\n",
        "            else:\n",
        "                print(f\"Directory does NOT exist!\")\n",
        "\n",
        "        print(\"Initializing models...\")\n",
        "\n",
        "        # Inizializza il modello di riconoscimento vocale Whisper\n",
        "        self.transcriber = self._load_whisper_model()\n",
        "\n",
        "        # Inizializza il modello di traduzione MBart\n",
        "        self.translator_model, self.translator_tokenizer = self._load_translation_model()\n",
        "\n",
        "        # Controlla la disponibilità della GPU\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {device}\")\n",
        "        if device.type == 'cpu':\n",
        "            print(\"WARNING: No GPU detected. Tortoise-TTS may be very slow on CPU.\")\n",
        "\n",
        "        # Inizializza Tortoise TTS per il clonaggio vocale\n",
        "        self.tts = TextToSpeech(device=device)\n",
        "\n",
        "        # Mappa dei codici lingua per MBart\n",
        "        self.lang_map = {\n",
        "            \"it\": \"it_IT\",  # Italiano\n",
        "            \"en\": \"en_XX\",  # Inglese\n",
        "            \"fr\": \"fr_XX\",  # Francese\n",
        "            \"es\": \"es_XX\",  # Spagnolo\n",
        "            \"de\": \"de_DE\",  # Tedesco\n",
        "            \"zh\": \"zh_CN\",  # Cinese\n",
        "            \"ru\": \"ru_RU\",  # Russo\n",
        "            \"ja\": \"ja_XX\",  # Giapponese\n",
        "            \"pt\": \"pt_XX\",  # Portoghese\n",
        "            \"ar\": \"ar_AR\",  # Arabo\n",
        "            \"hi\": \"hi_IN\"   # Hindi\n",
        "        }\n",
        "\n",
        "        # Inizializza le variabili per il condizionamento vocale\n",
        "        self.voice_samples = None\n",
        "        self.conditioning_latents = None\n",
        "\n",
        "        # Carica la voce dalla cache se disponibile\n",
        "        if self.use_cache and self.voice_samples_dir:\n",
        "            self._load_cached_voice()\n",
        "\n",
        "        print(\"Models loaded successfully.\")\n",
        "\n",
        "    def _load_whisper_model(self):\n",
        "        \"\"\"Carica il modello Whisper, utilizzando la cache se disponibile.\"\"\"\n",
        "        model_cache_path = os.path.join(self.cache_dir, \"whisper_model.pkl\")\n",
        "\n",
        "        if self.use_cache and os.path.exists(model_cache_path):\n",
        "            try:\n",
        "                print(\"Loading Whisper model from cache...\")\n",
        "                with open(model_cache_path, 'rb') as f:\n",
        "                    model = pickle.load(f)\n",
        "                return model\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading cached Whisper model: {e}\")\n",
        "                print(\"Loading fresh model...\")\n",
        "\n",
        "        model = whisper.load_model(\"medium\")\n",
        "\n",
        "        # Salva il modello in cache\n",
        "        if self.use_cache:\n",
        "            try:\n",
        "                with open(model_cache_path, 'wb') as f:\n",
        "                    pickle.dump(model, f)\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to cache Whisper model: {e}\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    def _load_translation_model(self):\n",
        "        \"\"\"Carica il modello di traduzione MBart, utilizzando la cache se disponibile.\"\"\"\n",
        "        model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        "\n",
        "        tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
        "        model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "        return model, tokenizer\n",
        "\n",
        "    def _get_voice_cache_path(self):\n",
        "        \"\"\"Genera un percorso unico per la cache basato sui campioni vocali.\"\"\"\n",
        "        if not self.voice_samples_dir:\n",
        "            return None\n",
        "\n",
        "        # Crea un hash del contenuto della directory dei campioni vocali\n",
        "        hash_obj = hashlib.md5()\n",
        "        wav_files = sorted([f for f in os.listdir(self.voice_samples_dir) if f.endswith('.wav')])\n",
        "\n",
        "        for wav_file in wav_files:\n",
        "            file_path = os.path.join(self.voice_samples_dir, wav_file)\n",
        "            file_stat = os.stat(file_path)\n",
        "            hash_obj.update(f\"{wav_file}_{file_stat.st_size}_{file_stat.st_mtime}\".encode())\n",
        "\n",
        "        dir_hash = hash_obj.hexdigest()\n",
        "        return os.path.join(self.cache_dir, f\"voice_latents_{dir_hash}.pt\")\n",
        "\n",
        "    def _load_cached_voice(self):\n",
        "        \"\"\"Carica i latenti vocali dalla cache se disponibili.\"\"\"\n",
        "        cache_path = self._get_voice_cache_path()\n",
        "\n",
        "        if not cache_path:\n",
        "            return False\n",
        "\n",
        "        if os.path.exists(cache_path):\n",
        "            try:\n",
        "                print(f\"Loading cached voice from {cache_path}\")\n",
        "                cached_data = torch.load(cache_path)\n",
        "                self.voice_samples = cached_data.get('voice_samples')\n",
        "                self.conditioning_latents = cached_data.get('conditioning_latents')\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading cached voice: {e}\")\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _save_voice_to_cache(self, voice_samples, conditioning_latents):\n",
        "        \"\"\"Salva i latenti vocali nella cache.\"\"\"\n",
        "        if not self.use_cache:\n",
        "            return\n",
        "\n",
        "        cache_path = self._get_voice_cache_path()\n",
        "        if not cache_path:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            print(f\"Saving voice to cache: {cache_path}\")\n",
        "            cache_data = {\n",
        "                'voice_samples': voice_samples,\n",
        "                'conditioning_latents': conditioning_latents\n",
        "            }\n",
        "            torch.save(cache_data, cache_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving voice to cache: {e}\")\n",
        "\n",
        "    def clear_cache(self, voice_only=False):\n",
        "        \"\"\"Pulisce la directory della cache.\n",
        "\n",
        "        Args:\n",
        "            voice_only: Se True, pulisce solo la cache vocale\n",
        "        \"\"\"\n",
        "        if voice_only:\n",
        "            print(\"Clearing voice cache...\")\n",
        "            voice_cache_files = [f for f in os.listdir(self.cache_dir) if f.startswith(\"voice_latents_\")]\n",
        "            for file in voice_cache_files:\n",
        "                try:\n",
        "                    os.remove(os.path.join(self.cache_dir, file))\n",
        "                except Exception as e:\n",
        "                    print(f\"Error removing {file}: {e}\")\n",
        "        else:\n",
        "            print(\"Clearing all cache...\")\n",
        "            for file in os.listdir(self.cache_dir):\n",
        "                try:\n",
        "                    file_path = os.path.join(self.cache_dir, file)\n",
        "                    if os.path.isfile(file_path):\n",
        "                        os.remove(file_path)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error removing {file}: {e}\")\n",
        "\n",
        "    def extract_audio(self, video_path):\n",
        "        \"\"\"Estrae l'audio dal video.\"\"\"\n",
        "        print(\"Extracting audio from video...\")\n",
        "        audio_path = os.path.join(self.temp_dir, \"extracted_audio.wav\")\n",
        "        video = VideoFileClip(video_path)\n",
        "        video.audio.write_audiofile(audio_path, codec='pcm_s16le', fps=16000)\n",
        "        return audio_path\n",
        "\n",
        "    def transcribe_audio(self, audio_path):\n",
        "        \"\"\"Trascrive l'audio utilizzando Whisper.\"\"\"\n",
        "        print(\"Transcribing audio...\")\n",
        "        result = self.transcriber.transcribe(audio_path, language=self.source_lang)\n",
        "        return result\n",
        "\n",
        "    def translate_text(self, text):\n",
        "        \"\"\"Traduce il testo nella lingua target utilizzando MBart.\"\"\"\n",
        "        print(\"Translating text...\")\n",
        "        self.translator_tokenizer.src_lang = self.lang_map[self.source_lang]\n",
        "\n",
        "        # Tokenizza il testo\n",
        "        encoded = self.translator_tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "        # Genera la traduzione\n",
        "        generated_tokens = self.translator_model.generate(\n",
        "            **encoded,\n",
        "            forced_bos_token_id=self.translator_tokenizer.lang_code_to_id[self.lang_map[self.target_lang]]\n",
        "        )\n",
        "\n",
        "        # Decodifica la traduzione\n",
        "        translation = self.translator_tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
        "        return translation\n",
        "\n",
        "    def align_segments(self, transcription, translation):\n",
        "        \"\"\"Allinea i segmenti tradotti con i tempi originali.\"\"\"\n",
        "        print(\"Aligning segments...\")\n",
        "        aligned_segments = []\n",
        "        segments = transcription[\"segments\"]\n",
        "\n",
        "        # Suddivide la traduzione in segmenti proporzionali agli originali\n",
        "        for segment in segments:\n",
        "            original_text = segment[\"text\"]\n",
        "            start_time = segment[\"start\"]\n",
        "            end_time = segment[\"end\"]\n",
        "\n",
        "            # Ottiene la traduzione del segmento\n",
        "            translation_segment = self.translate_text(original_text)\n",
        "\n",
        "            aligned_segments.append({\n",
        "                \"start\": start_time,\n",
        "                \"end\": end_time,\n",
        "                \"text\": translation_segment\n",
        "            })\n",
        "\n",
        "        return aligned_segments\n",
        "\n",
        "    def prepare_voice_samples(self):\n",
        "        \"\"\"Prepara i campioni vocali per il clonaggio.\"\"\"\n",
        "        if self.conditioning_latents is not None and self.voice_samples is not None:\n",
        "            print(\"Using cached voice conditioning latents\")\n",
        "            return self.voice_samples, self.conditioning_latents\n",
        "\n",
        "        if not self.voice_samples_dir or not os.path.exists(self.voice_samples_dir):\n",
        "            print(\"Voice samples directory not found.\")\n",
        "            return None, None\n",
        "\n",
        "        wav_files = [f for f in os.listdir(self.voice_samples_dir) if f.endswith('.wav')]\n",
        "        if not wav_files:\n",
        "            print(\"No WAV files found in voice samples directory.\")\n",
        "            return None, None\n",
        "\n",
        "        print(f\"Loading {len(wav_files)} voice samples...\")\n",
        "\n",
        "        # Carica i campioni vocali\n",
        "        voice_samples = []\n",
        "        for wav_file in wav_files:\n",
        "            file_path = os.path.join(self.voice_samples_dir, wav_file)\n",
        "            try:\n",
        "                # Carica audio a 24kHz (usato da Tortoise)\n",
        "                audio, sr = librosa.load(file_path, sr=24000, mono=True)\n",
        "                # Converte in tensor e aggiunge dimensione batch\n",
        "                audio_tensor = torch.FloatTensor(audio).unsqueeze(0).to(self.tts.device)\n",
        "                voice_samples.append(audio_tensor)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading file {wav_file}: {e}\")\n",
        "\n",
        "        if not voice_samples:\n",
        "            print(\"No valid voice samples loaded.\")\n",
        "            return None, None\n",
        "\n",
        "        # Genera i latenti per il condizionamento una sola volta\n",
        "        print(\"Generating voice conditioning latents...\")\n",
        "        try:\n",
        "            conditioning_latents = self.tts.get_conditioning_latents(voice_samples)\n",
        "\n",
        "            # Salva in cache per usi futuri\n",
        "            self._save_voice_to_cache(voice_samples, conditioning_latents)\n",
        "\n",
        "            # Memorizza nell'istanza per riutilizzo\n",
        "            self.voice_samples = voice_samples\n",
        "            self.conditioning_latents = conditioning_latents\n",
        "\n",
        "            return voice_samples, conditioning_latents\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating voice conditioning latents: {e}\")\n",
        "            print(traceback.format_exc())\n",
        "            return None, None\n",
        "\n",
        "    def clone_voice(self, text, segment_idx=0):\n",
        "        \"\"\"Genera audio con voce clonata per il segmento specificato.\"\"\"\n",
        "        print(f\"Generating audio for segment {segment_idx + 1}...\")\n",
        "\n",
        "        try:\n",
        "            # Prepara il condizionamento vocale se non già fatto\n",
        "            if self.conditioning_latents is None:\n",
        "                voice_samples, conditioning_latents = self.prepare_voice_samples()\n",
        "            else:\n",
        "                voice_samples, conditioning_latents = self.voice_samples, self.conditioning_latents\n",
        "\n",
        "            if conditioning_latents is not None:\n",
        "                print(f\"Using cached voice conditioning for generation...\")\n",
        "                # Utilizza i latenti di condizionamento in cache\n",
        "                gen = self.tts.tts_with_preset(text,\n",
        "                                              conditioning_latents=conditioning_latents,\n",
        "                                              preset=\"fast\")  # Options: ultra_fast, fast, standard, high_quality\n",
        "            else:\n",
        "                print(\"Using default voice...\")\n",
        "                gen = self.tts.tts(text, voice_samples=None)\n",
        "\n",
        "            # Ottiene il primo risultato\n",
        "            audio = gen[0].cpu().numpy()\n",
        "            return audio\n",
        "        except Exception as e:\n",
        "            print(f\"Error during audio generation: {e}\")\n",
        "            print(traceback.format_exc())\n",
        "\n",
        "            # Crea audio silenzioso come fallback\n",
        "            print(\"Creating silent audio as fallback...\")\n",
        "            return np.zeros(int(24000 * 3))  # 3 secondi di silenzio a 24kHz\n",
        "\n",
        "    def generate_audio_segments(self, aligned_segments):\n",
        "        \"\"\"Genera segmenti audio per ogni segmento tradotto.\"\"\"\n",
        "        print(\"Generating audio segments...\")\n",
        "        audio_segments = []\n",
        "\n",
        "        for i, segment in enumerate(aligned_segments):\n",
        "            text = segment[\"text\"]\n",
        "            start_time = segment[\"start\"]\n",
        "            end_time = segment[\"end\"]\n",
        "            duration = end_time - start_time\n",
        "\n",
        "            # Genera audio con voce clonata\n",
        "            audio = self.clone_voice(text, i)\n",
        "\n",
        "            # Salva l'audio temporaneamente con numpy.save\n",
        "            temp_audio_path = os.path.join(self.temp_dir, f\"segment_{i}.npy\")\n",
        "            np.save(temp_audio_path, audio)\n",
        "\n",
        "            # Converte l'array numpy in WAV usando ffmpeg\n",
        "            wav_path = os.path.join(self.temp_dir, f\"segment_{i}.wav\")\n",
        "            temp_raw_path = os.path.join(self.temp_dir, f\"segment_{i}.raw\")\n",
        "\n",
        "            try:\n",
        "                # Salva prima come dati PCM raw\n",
        "                with open(temp_raw_path, 'wb') as f:\n",
        "                    # Converte in 16-bit PCM\n",
        "                    audio_16bit = (audio * 32767).astype(np.int16)\n",
        "                    audio_16bit.tofile(f)\n",
        "\n",
        "                # Usa ffmpeg per convertire in WAV\n",
        "                result = subprocess.call([\n",
        "                    \"ffmpeg\", \"-y\",\n",
        "                    \"-f\", \"s16le\",  # 16-bit signed little endian\n",
        "                    \"-ar\", \"24000\",  # frequenza 24kHz\n",
        "                    \"-ac\", \"1\",      # 1 canale (mono)\n",
        "                    \"-i\", temp_raw_path,\n",
        "                    wav_path\n",
        "                ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "                if result != 0 or not os.path.exists(wav_path):\n",
        "                    print(f\"ERROR: Failed to create WAV file for segment {i}\")\n",
        "                    continue\n",
        "\n",
        "                # Carica l'audio per regolare la velocità\n",
        "                audio_segment = AudioSegment.from_file(wav_path)\n",
        "\n",
        "                # Calcola il fattore di velocità per il sincronismo labiale\n",
        "                current_duration = len(audio_segment) / 1000.0  # durata in secondi\n",
        "                speed_factor = current_duration / duration if duration > 0 else 1.0\n",
        "\n",
        "                # Regola la velocità se necessario\n",
        "                if abs(speed_factor - 1.0) > 0.1:  # Differenza significativa\n",
        "                    if speed_factor > 1.5:  # Limita l'accelerazione\n",
        "                        speed_factor = 1.5\n",
        "\n",
        "                    # Usa ffmpeg per regolare la velocità mantenendo il pitch\n",
        "                    adjusted_path = os.path.join(self.temp_dir, f\"adjusted_{i}.wav\")\n",
        "                    result = subprocess.call([\n",
        "                        \"ffmpeg\", \"-y\", \"-i\", wav_path,\n",
        "                        \"-filter:a\", f\"atempo={speed_factor}\",\n",
        "                        adjusted_path\n",
        "                    ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "                    if result != 0 or not os.path.exists(adjusted_path):\n",
        "                        print(f\"ERROR: Failed to adjust audio speed for segment {i}\")\n",
        "                        adjusted_path = wav_path\n",
        "                    else:\n",
        "                        audio_segment = AudioSegment.from_file(adjusted_path)\n",
        "                else:\n",
        "                    adjusted_path = wav_path\n",
        "\n",
        "                try:\n",
        "                    if os.path.exists(temp_raw_path):\n",
        "                        os.remove(temp_raw_path)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "                audio_segments.append({\n",
        "                    \"path\": adjusted_path,\n",
        "                    \"start\": start_time,\n",
        "                    \"end\": end_time,\n",
        "                    \"duration\": len(audio_segment) / 1000.0\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating audio segment {i}: {e}\")\n",
        "                print(traceback.format_exc())\n",
        "\n",
        "        return audio_segments\n",
        "\n",
        "    def combine_audio_segments(self, audio_segments, original_audio_path):\n",
        "        \"\"\"Combina i segmenti audio utilizzando la separazione vocale AI.\"\"\"\n",
        "        print(\"Combining audio segments with AI voice separation...\")\n",
        "\n",
        "        if not audio_segments:\n",
        "            print(\"WARNING: No audio segments to combine.\")\n",
        "            return original_audio_path\n",
        "\n",
        "        # Step 1: Usa Demucs per separare voce e musica/fondo nell'audio originale\n",
        "        print(\"Separating voice and background from original audio...\")\n",
        "\n",
        "        try:\n",
        "            demucs_output_dir = os.path.join(self.temp_dir, \"demucs_output\")\n",
        "            os.makedirs(demucs_output_dir, exist_ok=True)\n",
        "\n",
        "            wav_original = os.path.join(self.temp_dir, \"original_for_separation.wav\")\n",
        "\n",
        "            if not original_audio_path.lower().endswith('.wav'):\n",
        "                subprocess.call([\n",
        "                    \"ffmpeg\", \"-y\", \"-i\", original_audio_path, wav_original\n",
        "                ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "            else:\n",
        "                shutil.copy(original_audio_path, wav_original)\n",
        "\n",
        "            print(\"Running Demucs AI separation...\")\n",
        "\n",
        "            demucs.separate.main([\"--out\", demucs_output_dir, \"--two-stems\", \"vocals\", wav_original])\n",
        "\n",
        "            htdemucs_dir = os.path.join(demucs_output_dir, \"htdemucs\")\n",
        "            if not os.path.exists(htdemucs_dir):\n",
        "                possible_dirs = [d for d in os.listdir(demucs_output_dir) if os.path.isdir(os.path.join(demucs_output_dir, d))]\n",
        "                if possible_dirs:\n",
        "                    htdemucs_dir = os.path.join(demucs_output_dir, possible_dirs[0])\n",
        "                else:\n",
        "                    raise FileNotFoundError(\"Could not find Demucs output directory\")\n",
        "\n",
        "            base_name = os.path.basename(wav_original).split('.')[0]\n",
        "            no_vocals_path = os.path.join(htdemucs_dir, base_name, \"no_vocals.wav\")\n",
        "            vocals_path = os.path.join(htdemucs_dir, base_name, \"vocals.wav\")\n",
        "\n",
        "            if not (os.path.exists(no_vocals_path) and os.path.exists(vocals_path)):\n",
        "                raise FileNotFoundError(f\"Demucs output files not found. Looking for: {no_vocals_path} and {vocals_path}\")\n",
        "\n",
        "            print(\"Successfully separated voice and background!\")\n",
        "\n",
        "            background_audio = AudioSegment.from_file(no_vocals_path)\n",
        "            final_audio = AudioSegment.silent(duration=len(background_audio))\n",
        "\n",
        "            for segment in audio_segments:\n",
        "                try:\n",
        "                    path = segment[\"path\"]\n",
        "                    if os.path.exists(path):\n",
        "                        start_ms = int(segment[\"start\"] * 1000)\n",
        "                        audio = AudioSegment.from_file(path)\n",
        "                        final_audio = final_audio.overlay(audio, position=start_ms)\n",
        "                    else:\n",
        "                        print(f\"WARNING: Missing audio file: {path}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during audio segment combination: {e}\")\n",
        "\n",
        "            final_mixed_audio = background_audio.overlay(final_audio)\n",
        "\n",
        "            final_audio_path = os.path.join(self.temp_dir, \"final_audio.wav\")\n",
        "            final_mixed_audio.export(final_audio_path, format=\"wav\")\n",
        "\n",
        "            return final_audio_path\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in AI voice separation: {e}\")\n",
        "            print(traceback.format_exc())\n",
        "\n",
        "            print(\"Falling back to basic audio combination...\")\n",
        "\n",
        "            original_audio = AudioSegment.from_file(original_audio_path)\n",
        "            final_audio = AudioSegment.silent(duration=len(original_audio))\n",
        "\n",
        "            for segment in audio_segments:\n",
        "                try:\n",
        "                    path = segment[\"path\"]\n",
        "                    if os.path.exists(path):\n",
        "                        start_ms = int(segment[\"start\"] * 1000)\n",
        "                        audio = AudioSegment.from_file(path)\n",
        "                        final_audio = final_audio.overlay(audio, position=start_ms)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error: {e}\")\n",
        "\n",
        "            final_audio_path = os.path.join(self.temp_dir, \"final_audio.wav\")\n",
        "            final_audio.export(final_audio_path, format=\"wav\")\n",
        "\n",
        "            return final_audio_path\n",
        "\n",
        "    def combine_video_and_audio(self, video_path, audio_path, output_path):\n",
        "        \"\"\"Combina il video originale con l'audio tradotto.\"\"\"\n",
        "        print(\"Combining video and audio...\")\n",
        "        command = [\n",
        "            \"ffmpeg\", \"-y\",\n",
        "            \"-i\", video_path,\n",
        "            \"-i\", audio_path,\n",
        "            \"-c:v\", \"copy\",\n",
        "            \"-c:a\", \"aac\",\n",
        "            \"-map\", \"0:v:0\",\n",
        "            \"-map\", \"1:a:0\",\n",
        "            output_path\n",
        "        ]\n",
        "        subprocess.call(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "        return output_path\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Pulisce i file temporanei.\"\"\"\n",
        "        print(\"Cleaning up temporary files...\")\n",
        "        # In questa versione manteniamo i file nella directory dedicata\n",
        "        # Non eliminiamo self.temp_dir\n",
        "        pass\n",
        "\n",
        "    def process_video(self, video_path, output_path):\n",
        "        \"\"\"Processa il video dall'inizio alla fine.\"\"\"\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            print(f\"Starting video processing: {video_path}\")\n",
        "\n",
        "            # Prepara il condizionamento vocale se applicabile\n",
        "            if self.voice_samples_dir:\n",
        "                self.prepare_voice_samples()\n",
        "\n",
        "            # Estrai l'audio dal video\n",
        "            audio_path = self.extract_audio(video_path)\n",
        "\n",
        "            # Trascrivi l'audio\n",
        "            transcription = self.transcribe_audio(audio_path)\n",
        "\n",
        "            # Salva la trascrizione per riferimento\n",
        "            transcription_path = os.path.join(self.temp_dir, \"transcription.json\")\n",
        "            with open(transcription_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(transcription, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "            # Allinea i segmenti tradotti\n",
        "            aligned_segments = self.align_segments(transcription, None)\n",
        "\n",
        "            # Salva i segmenti allineati per riferimento\n",
        "            aligned_path = os.path.join(self.temp_dir, \"aligned_segments.json\")\n",
        "            with open(aligned_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(aligned_segments, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "            # Genera i segmenti audio con voce clonata\n",
        "            audio_segments = self.generate_audio_segments(aligned_segments)\n",
        "\n",
        "            # Combina i segmenti audio\n",
        "            final_audio_path = self.combine_audio_segments(audio_segments, audio_path)\n",
        "\n",
        "            # Combina il video originale con l'audio tradotto\n",
        "            result_path = self.combine_video_and_audio(video_path, final_audio_path, output_path)\n",
        "\n",
        "            elapsed_time = time.time() - start_time\n",
        "            print(f\"Processing completed in {elapsed_time:.2f} seconds\")\n",
        "            print(f\"Translated video saved to: {output_path}\")\n",
        "\n",
        "            return result_path\n",
        "        except Exception as e:\n",
        "            print(f\"Error during video processing: {e}\")\n",
        "            print(traceback.format_exc())\n",
        "            return None\n",
        "        finally:\n",
        "            self.cleanup()\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='Video Translator with voice cloning and lip sync')\n",
        "    parser.add_argument('--input', required=True, help='Path to input video')\n",
        "    parser.add_argument('--output', required=True, help='Path to output video')\n",
        "    parser.add_argument('--source-lang', default='it', help='Source language (default: it)')\n",
        "    parser.add_argument('--target-lang', default='en', help='Target language (default: en)')\n",
        "    parser.add_argument('--voice-samples', help='Directory containing voice samples for voice cloning')\n",
        "    parser.add_argument('--no-cache', action='store_true', help='Disable caching of voice samples and models')\n",
        "    parser.add_argument('--clear-cache', action='store_true', help='Clear all cached data before processing')\n",
        "    parser.add_argument('--clear-voice-cache', action='store_true', help='Clear only voice cache before processing')\n",
        "    parser.add_argument('--keep-temp', action='store_true', help='Keep temporary files after processing')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Verifica che il file di input esista\n",
        "    if not os.path.exists(args.input):\n",
        "        print(f\"Error: Input file {args.input} does not exist.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # Crea la directory 'conversions' nella cartella dello script\n",
        "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
        "    conversions_dir = os.path.join(script_dir, \"conversions\")\n",
        "    os.makedirs(conversions_dir, exist_ok=True)\n",
        "\n",
        "    # Inizializza il video translator\n",
        "    translator = VideoTranslator(\n",
        "        source_lang=args.source_lang,\n",
        "        target_lang=args.target_lang,\n",
        "        voice_samples_dir=args.voice_samples,\n",
        "        input_video_path=args.input,\n",
        "        use_cache=not args.no_cache\n",
        "    )\n",
        "\n",
        "    if args.clear_cache:\n",
        "        translator.clear_cache(voice_only=False)\n",
        "    elif args.clear_voice_cache:\n",
        "        translator.clear_cache(voice_only=True)\n",
        "\n",
        "    translator.process_video(args.input, args.output)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "interactive-use-md"
      },
      "source": [
        "## Utilizzo Interattivo\n",
        "\n",
        "qui definiamo direttamente i percorsi e utilizziamo il metodo `process_video` in modo interattivo.\n",
        "\n",
        "Assicurati di caricare il video di input (ad es. `input_video.mp4`) e, se desiderato, la directory dei campioni vocali (`voice_samples`) nell'ambiente Colab.\n",
        "\n",
        "## Nota\n",
        "Il processo potrebbe richiedere del tempo, in base alla lunghezza del video e alle risorse disponibili.\n",
        "\n",
        "Al termine, il video tradotto verrà salvato nel percorso specificato in output_video_path e visualizzato in un player qui sotto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "interactive-use-code"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Modifica i seguenti percorsi secondo le tue esigenze\n",
        "input_video_path = '/content/data/input_video.mov'  # Carica il tuo video in Colab, tasto destro sulla file e \"mostra percorso\" per copiare il percorso esatto\n",
        "output_video_path = '/content/data/output_video.mov'\n",
        "voice_samples_directory = '/content/data/voice_samples'  # La directory deve contenere file .wav per la clonazione vocale\n",
        "\n",
        "# Inizializza il traduttore video\n",
        "translator = VideoTranslator(\n",
        "    source_lang='it',\n",
        "    target_lang='en',\n",
        "    voice_samples_dir=voice_samples_directory,\n",
        "    input_video_path=input_video_path,\n",
        "    use_cache=True\n",
        ")\n",
        "\n",
        "# Processa il video e genera il video tradotto\n",
        "translator.process_video(input_video_path, output_video_path)\n",
        "\n",
        "print(f\"Video tradotto salvato in: {output_video_path}\")\n",
        "\n",
        "# Leggi il file e codificalo in base64\n",
        "def get_video_html(video_path):\n",
        "    with open(output_video_path, \"rb\") as video_file:\n",
        "        video_base64 = base64.b64encode(video_file.read()).decode()\n",
        "    return f\"\"\"\n",
        "    <video width=\"640\" height=\"360\" controls>\n",
        "        <source src=\"data:video/mp4;base64,{video_base64}\" type=\"video/mp4\">\n",
        "        Your browser does not support the video tag.\n",
        "    </video>\n",
        "    \"\"\"\n",
        "\n",
        "# Mostra il video direttamente nel notebook\n",
        "HTML(get_video_html(output_video_path))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}